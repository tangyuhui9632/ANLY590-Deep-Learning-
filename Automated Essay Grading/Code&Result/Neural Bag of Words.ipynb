{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h6kR_BxUU-WO"
   },
   "source": [
    "# ANLY 590 - Automatic Scoring with Neural Bag of Words (Run on Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xbgsoeGU-WR"
   },
   "source": [
    "### Data Description\n",
    "\n",
    "The data is obtained from the Automated Student Assessment Prize (ASAP) AES dataset (https://www.kaggle.com/c/asap-aes/data), which contains essays written by students ranging from Grade 7 to Grade 10. The dataset consists of 8 essay sets, each with a different topic or prompt, with a total of 12,978 essays with scores.\n",
    "\n",
    "Each of the sets of essays was generated from a single prompt. Selected essays range from an average length of 150 to 550 words per response. Some of the essays are dependent upon source information and others are not. All responses were written by students ranging in grade levels from Grade 7 to Grade 10. All essays were hand graded and were double-scored. Each of the eight data sets has its own unique characteristics. The variability is intended to test the limits of your scoring engine's capabilities.\n",
    "\n",
    "The training data is provided in three formats: a tab-separated value (TSV) file, a Microsoft Excel 2010 spreadsheet, and a Microsoft Excel 2003 spreadsheet.  The current release of the training data contains essay sets 1-6.  Sets 7-8 will be released on February 10, 2012.  Each of these files contains 28 columns:\n",
    "\n",
    "    essay_id: A unique identifier for each individual student essay\n",
    "    essay_set: 1-8, an id for each set of essays\n",
    "    essay: The ascii text of a student's response\n",
    "    rater1_domain1: Rater 1's domain 1 score; all essays have this\n",
    "    rater2_domain1: Rater 2's domain 1 score; all essays have this\n",
    "    rater3_domain1: Rater 3's domain 1 score; only some essays in set 8 have this.\n",
    "    domain1_score: Resolved score between the raters; all essays have this\n",
    "    rater1_domain2: Rater 1's domain 2 score; only essays in set 2 have this\n",
    "    rater2_domain2: Rater 2's domain 2 score; only essays in set 2 have this\n",
    "    domain2_score: Resolved score between the raters; only essays in set 2 have this\n",
    "    rater1_trait1 score - rater3_trait6 score: trait scores for sets 7-8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Hem6UZEU-WR"
   },
   "source": [
    "### Setting up ML libraries\n",
    "\n",
    "Importing the relevant NLP and tensorflow libraries for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4zISP1oU-WS"
   },
   "outputs": [],
   "source": [
    "#!pip install constants\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import datetime\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Pandas and SKLearn\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Matplotlib and Seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#!pip install kashgari\n",
    "# Utilizing a package Kashgari to perform the pre-trained BERT embeddings\n",
    "from kashgari.embeddings import BERTEmbedding\n",
    "\n",
    "!pip install layers\n",
    "# Importing Keras packages\n",
    "from keras import initializers, regularizers, optimizers\n",
    "from keras.layers import LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "# import layers; reload(layers)\n",
    "# from layers import AttentionWithContext, Addition\n",
    "\n",
    "# Importing packages for Tensorboard for Keras\n",
    "from time import time\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "# XGBoost\n",
    "import xgboost\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o2jIAZildXw6"
   },
   "source": [
    "# **Layers:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DXPBMpU5cDpb"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Reshape, Flatten, LSTM, Dense, Dropout, Embedding, Bidirectional, GRU\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers, regularizers\n",
    "from keras import optimizers\n",
    "from keras.engine.topology import Layer\n",
    "from keras import constraints\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    follows these equations:\n",
    "\n",
    "    (1) u_t = tanh(W h_t + b)\n",
    "    (2) \\alpha_t = \\frac{exp(u^T u)}{\\sum_t(exp(u_t^T u))}, this is the attention weight\n",
    "    (3) v_t = \\alpha_t * h_t, v in time t\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero and this results in NaN's. \n",
    "        # Should add a small epsilon as the workaround\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "\n",
    "        return weighted_input\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[1], input_shape[2]\n",
    "    \n",
    "class Addition(Layer):\n",
    "    \"\"\"\n",
    "    This layer is supposed to add of all activation weight.\n",
    "    We split this from AttentionWithContext to help us getting the activation weights\n",
    "    follows this equation:\n",
    "    (1) v = \\sum_t(\\alpha_t * h_t)\n",
    "\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Addition, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.output_dim = input_shape[-1]\n",
    "        super(Addition, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.sum(x, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BjcOd0I4dnjm"
   },
   "source": [
    "# patched_numpy_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TYJnDGmadgen"
   },
   "outputs": [],
   "source": [
    "# Modified from tensorflow/python/estimator/inputs/numpy_io.py\n",
    "\n",
    "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Methods to allow dict of numpy arrays.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "from tensorflow.python.estimator.inputs.queues import feeding_functions\n",
    "\n",
    "# Key name to pack the target into dict of `features`. See\n",
    "# `_get_unique_target_key` for details.\n",
    "_TARGET_KEY = '__target_key__'\n",
    "\n",
    "\n",
    "def _get_unique_target_key(features):\n",
    "  \"\"\"Returns a key not existed in the input dict `features`.\n",
    "  Caller of `input_fn` usually provides `features` (dict of numpy arrays) and\n",
    "  `target`, but the underlying feeding module expects a single dict of numpy\n",
    "  arrays as input. So, the `target` needs to be packed into the `features`\n",
    "  temporarily and unpacked after calling the feeding function. Toward this goal,\n",
    "  this function returns a key not existed in the `features` to pack the\n",
    "  `target`.\n",
    "  \"\"\"\n",
    "  target_key = _TARGET_KEY\n",
    "  while target_key in features:\n",
    "    target_key += '_n'\n",
    "  return target_key\n",
    "\n",
    "\n",
    "def numpy_input_fn(x,\n",
    "                   y=None,\n",
    "                   batch_size=128,\n",
    "                   num_epochs=1,\n",
    "                   shuffle=None,\n",
    "\t\t\t\t   seed=42,\n",
    "                   queue_capacity=1000,\n",
    "                   num_threads=1):\n",
    "  \"\"\"Returns input function that would feed dict of numpy arrays into the model.\n",
    "  This returns a function outputting `features` and `target` based on the dict\n",
    "  of numpy arrays. The dict `features` has the same keys as the `x`.\n",
    "  Example:\n",
    "  ```python\n",
    "  age = np.arange(4) * 1.0\n",
    "  height = np.arange(32, 36)\n",
    "  x = {'age': age, 'height': height}\n",
    "  y = np.arange(-32, -28)\n",
    "  with tf.Session() as session:\n",
    "    input_fn = numpy_io.numpy_input_fn(\n",
    "        x, y, batch_size=2, shuffle=False, num_epochs=1)\n",
    "  ```\n",
    "  Args:\n",
    "    x: dict of numpy array object.\n",
    "    y: numpy array object. `None` if absent.\n",
    "    batch_size: Integer, size of batches to return.\n",
    "    num_epochs: Integer, number of epochs to iterate over data. If `None` will\n",
    "      run forever.\n",
    "    shuffle: Boolean, if True shuffles the queue. Avoid shuffle at prediction\n",
    "      time.\n",
    "\tseed: seed for RNG, to use if shuffle=True.\n",
    "    queue_capacity: Integer, size of queue to accumulate.\n",
    "    num_threads: Integer, number of threads used for reading and enqueueing. In\n",
    "      order to have predicted and repeatable order of reading and enqueueing,\n",
    "      such as in prediction and evaluation mode, `num_threads` should be 1.\n",
    "  Returns:\n",
    "    Function, that has signature of ()->(dict of `features`, `target`)\n",
    "  Raises:\n",
    "    ValueError: if the shape of `y` mismatches the shape of values in `x` (i.e.,\n",
    "      values in `x` have same shape).\n",
    "    TypeError: `x` is not a dict or `shuffle` is not bool.\n",
    "  \"\"\"\n",
    "\n",
    "  if not isinstance(shuffle, bool):\n",
    "    raise TypeError('shuffle must be explicitly set as boolean; '\n",
    "                    'got {}'.format(shuffle))\n",
    "\n",
    "  def input_fn():\n",
    "    \"\"\"Numpy input function.\"\"\"\n",
    "    if not isinstance(x, dict):\n",
    "      raise TypeError('x must be dict; got {}'.format(type(x).__name__))\n",
    "\n",
    "    # Make a shadow copy and also ensure the order of iteration is consistent.\n",
    "    ordered_dict_x = collections.OrderedDict(\n",
    "        sorted(x.items(), key=lambda t: t[0]))\n",
    "\n",
    "    unique_target_key = _get_unique_target_key(ordered_dict_x)\n",
    "    if y is not None:\n",
    "      ordered_dict_x[unique_target_key] = y\n",
    "\n",
    "    if len(set(v.shape[0] for v in ordered_dict_x.values())) != 1:\n",
    "      shape_dict_of_x = {k: ordered_dict_x[k].shape\n",
    "                         for k in ordered_dict_x.keys()}\n",
    "      shape_of_y = None if y is None else y.shape\n",
    "      raise ValueError('Length of tensors in x and y is mismatched. All '\n",
    "                       'elements in x and y must have the same length.\\n'\n",
    "                       'Shapes in x: {}\\n'\n",
    "                       'Shape for y: {}\\n'.format(shape_dict_of_x, shape_of_y))\n",
    "\n",
    "    queue = feeding_functions._enqueue_data(  # pylint: disable=protected-access\n",
    "        ordered_dict_x,\n",
    "        queue_capacity,\n",
    "        shuffle=shuffle,\n",
    "\t\tseed=seed,\n",
    "        num_threads=num_threads,\n",
    "        enqueue_size=batch_size,\n",
    "        num_epochs=num_epochs)\n",
    "\n",
    "    features = (queue.dequeue_many(batch_size) if num_epochs is None\n",
    "                else queue.dequeue_up_to(batch_size))\n",
    "\n",
    "    # Remove the first `Tensor` in `features`, which is the row number.\n",
    "    if len(features) > 0:\n",
    "      features.pop(0)\n",
    "\n",
    "    features = dict(zip(ordered_dict_x.keys(), features))\n",
    "    if y is not None:\n",
    "      target = features.pop(unique_target_key)\n",
    "      return features, target\n",
    "    return features\n",
    "\n",
    "  return input_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "czBSPMSpduTx"
   },
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FbdRz53JdgrM"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import re\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# For pretty-printing\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "#from . import constants\n",
    "import constants\n",
    "\n",
    "##\n",
    "# Package and module utils\n",
    "def require_package(package_name):\n",
    "    import pkgutil\n",
    "    import subprocess\n",
    "    import sys\n",
    "    if not pkgutil.find_loader(package_name):\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package_name])\n",
    "\n",
    "def run_tests(test_module, test_names, reload=True):\n",
    "    import unittest\n",
    "    if reload:\n",
    "        import importlib\n",
    "        importlib.reload(test_module)\n",
    "    unittest.TextTestRunner(verbosity=2).run(\n",
    "        unittest.TestLoader().loadTestsFromNames(\n",
    "            test_names, test_module))\n",
    "\n",
    "##\n",
    "# Miscellaneous helpers\n",
    "def flatten(list_of_lists):\n",
    "    \"\"\"Flatten a list-of-lists into a single list.\"\"\"\n",
    "    return list(itertools.chain.from_iterable(list_of_lists))\n",
    "\n",
    "def render_matrix(M, rows=None, cols=None, dtype=float, float_fmt=\"{0:.04f}\"):\n",
    "    \"\"\"Render a matrix to HTML using Pandas.\n",
    "\n",
    "    Args:\n",
    "      M : 2D numpy array\n",
    "      rows : list of row labels\n",
    "      cols : list of column labels\n",
    "      dtype : data type (float or int)\n",
    "      float_fmt : format specifier for floats\n",
    "\n",
    "    Returns:\n",
    "      (string) HTML representation of M\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(M, index=rows, columns=cols, dtype=dtype)\n",
    "    old_fmt_fn = pd.get_option('float_format')\n",
    "    pd.set_option('float_format', lambda f: float_fmt.format(f))\n",
    "    html = df._repr_html_()\n",
    "    pd.set_option('float_format', old_fmt_fn)  # reset Pandas formatting\n",
    "    return html\n",
    "\n",
    "def pretty_print_matrix(*args, **kw):\n",
    "    \"\"\"Pretty-print a matrix using Pandas.\n",
    "\n",
    "    Args:\n",
    "      M : 2D numpy array\n",
    "      rows : list of row labels\n",
    "      cols : list of column labels\n",
    "      dtype : data type (float or int)\n",
    "      float_fmt : format specifier for floats\n",
    "    \"\"\"\n",
    "    display(HTML(render_matrix(*args, **kw)))\n",
    "\n",
    "\n",
    "def pretty_timedelta(fmt=\"%d:%02d:%02d\", since=None, until=None):\n",
    "    \"\"\"Pretty-print a timedelta, using the given format string.\"\"\"\n",
    "    since = since or time.time()\n",
    "    until = until or time.time()\n",
    "    delta_s = until - since\n",
    "    hours, remainder = divmod(delta_s, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return fmt % (hours, minutes, seconds)\n",
    "\n",
    "\n",
    "##\n",
    "# Word processing functions\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset):\n",
    "        return word\n",
    "    else:\n",
    "        return constants.UNK_TOKEN\n",
    "\n",
    "def canonicalize_words(words, **kw):\n",
    "    return [canonicalize_word(word, **kw) for word in words]\n",
    "\n",
    "##\n",
    "# Data loading functions\n",
    "def get_corpus(name):\n",
    "    import nltk\n",
    "    assert(nltk.download(name))\n",
    "    return nltk.corpus.__getattr__(name)\n",
    "\n",
    "def build_vocab(corpus, V=10000, **kw):\n",
    "    from . import vocabulary\n",
    "    if isinstance(corpus, list):\n",
    "        token_feed = (canonicalize_word(w) for w in corpus)\n",
    "        vocab = vocabulary.Vocabulary(token_feed, size=V, **kw)\n",
    "    else:\n",
    "        token_feed = (canonicalize_word(w) for w in corpus.words())\n",
    "        vocab = vocabulary.Vocabulary(token_feed, size=V, **kw)\n",
    "\n",
    "    print(\"Vocabulary: {:,} types\".format(vocab.size))\n",
    "    return vocab\n",
    "\n",
    "def get_train_test_sents(corpus, split=0.8, shuffle=True):\n",
    "    \"\"\"Generate train/test split for unsupervised tasks.\n",
    "\n",
    "    Args:\n",
    "      corpus: nltk.corpus that supports sents() function\n",
    "      split (double): fraction to use as training set\n",
    "      shuffle (int or bool): seed for shuffle of input data, or False to just\n",
    "      take the training data as the first xx% contiguously.\n",
    "\n",
    "    Returns:\n",
    "      train_sentences, test_sentences ( list(list(string)) ): the train and test\n",
    "      splits\n",
    "    \"\"\"\n",
    "    sentences = np.array(list(corpus.sents()), dtype=object)\n",
    "    fmt = (len(sentences), sum(map(len, sentences)))\n",
    "    print(\"Loaded {:,} sentences ({:g} tokens)\".format(*fmt))\n",
    "\n",
    "    if shuffle:\n",
    "        rng = np.random.RandomState(shuffle)\n",
    "        rng.shuffle(sentences)  # in-place\n",
    "    split_idx = int(split * len(sentences))\n",
    "    train_sentences = sentences[:split_idx]\n",
    "    test_sentences = sentences[split_idx:]\n",
    "\n",
    "    fmt = (len(train_sentences), sum(map(len, train_sentences)))\n",
    "    print(\"Training set: {:,} sentences ({:,} tokens)\".format(*fmt))\n",
    "    fmt = (len(test_sentences), sum(map(len, test_sentences)))\n",
    "    print(\"Test set: {:,} sentences ({:,} tokens)\".format(*fmt))\n",
    "\n",
    "    return train_sentences, test_sentences\n",
    "\n",
    "def preprocess_sentences(sentences, vocab, use_eos=False, emit_ids=True,\n",
    "                         progressbar=lambda l:l):\n",
    "    \"\"\"Preprocess sentences by canonicalizing and mapping to ids.\n",
    "\n",
    "    Args:\n",
    "      sentences ( list(list(string)) ): input sentences\n",
    "      vocab: Vocabulary object, already initialized\n",
    "      use_eos: if true, will add </s> token to end of sentence.\n",
    "      emit_ids: if true, will emit as ids. Otherwise, will be preprocessed\n",
    "          tokens.\n",
    "      progressbar: (optional) progress bar to wrap iterator.\n",
    "\n",
    "    Returns:\n",
    "      ids ( array(int) ): flattened array of sentences, including boundary <s>\n",
    "      tokens.\n",
    "    \"\"\"\n",
    "    # Add sentence boundaries, canonicalize, and handle unknowns\n",
    "    word_preproc = lambda w: canonicalize_word(w, wordset=vocab.word_to_id)\n",
    "    ret = []\n",
    "    for s in progressbar(sentences):\n",
    "        canonical_words = vocab.pad_sentence(list(map(word_preproc, s)),\n",
    "                                             use_eos=use_eos)\n",
    "        ret.extend(vocab.words_to_ids(canonical_words) if emit_ids else\n",
    "                   canonical_words)\n",
    "    if not use_eos:  # add additional <s> to end if needed\n",
    "        ret.append(vocab.START_ID if emit_ids else vocab.START_TOKEN)\n",
    "    return np.array(ret, dtype=(np.int32 if emit_ids else object))\n",
    "\n",
    "\n",
    "def load_corpus(corpus, split=0.8, V=10000, shuffle=0):\n",
    "    \"\"\"Load a named corpus and split train/test along sentences.\n",
    "\n",
    "    This is a convenience wrapper to chain together several functions from this\n",
    "    module, and produce a train/test split suitable for input to most models.\n",
    "\n",
    "    Sentences are preprocessed by canonicalization and converted to ids\n",
    "    according to the constructed vocabulary, and interspersed with <s> tokens\n",
    "    to denote sentence bounaries.\n",
    "\n",
    "    Args:\n",
    "        corpus: (string | corpus reader) If a string, will fetch the\n",
    "            NLTK corpus of that name.\n",
    "        split: (float \\in (0,1]) fraction of examples in train split\n",
    "        V: (int) vocabulary size (including special tokens)\n",
    "        shuffle: (int) if > 0, use as random seed to shuffle sentence prior to\n",
    "            split. Can change this to get different splits.\n",
    "\n",
    "    Returns:\n",
    "        (vocab, train_ids, test_ids)\n",
    "        vocab: vocabulary.Vocabulary object\n",
    "        train_ids: flat (1D) np.array(int) of ids\n",
    "        test_ids: flat (1D) np.array(int) of ids\n",
    "    \"\"\"\n",
    "    if isinstance(corpus, str):\n",
    "        corpus = get_corpus(corpus)\n",
    "    vocab = build_vocab(corpus, V)\n",
    "    train_sentences, test_sentences = get_train_test_sents(corpus, split, shuffle)\n",
    "    train_ids = preprocess_sentences(train_sentences, vocab)\n",
    "    test_ids = preprocess_sentences(test_sentences, vocab)\n",
    "    return vocab, train_ids, test_ids\n",
    "\n",
    "##\n",
    "# Window and batch functions\n",
    "def pad_np_array(example_ids, max_len=250, pad_id=0):\n",
    "    \"\"\"Pad a list of lists of ids into a rectangular NumPy array.\n",
    "\n",
    "    Longer sequences will be truncated to max_len ids, while shorter ones will\n",
    "    be padded with pad_id.\n",
    "\n",
    "    Args:\n",
    "        example_ids: list(list(int)), sequence of ids for each example\n",
    "        max_len: maximum sequence length\n",
    "        pad_id: id to pad shorter sequences with\n",
    "\n",
    "    Returns: (x, ns)\n",
    "        x: [num_examples, max_len] NumPy array of integer ids\n",
    "        ns: [num_examples] NumPy array of sequence lengths (<= max_len)\n",
    "    \"\"\"\n",
    "    arr = np.full([len(example_ids), max_len], pad_id, dtype=np.int32)\n",
    "    ns = np.zeros([len(example_ids)], dtype=np.int32)\n",
    "    for i, ids in enumerate(example_ids):\n",
    "        cpy_len = min(len(ids), max_len)\n",
    "        arr[i,:cpy_len] = ids[:cpy_len]\n",
    "        ns[i] = cpy_len\n",
    "    return arr, ns\n",
    "\n",
    "def id_lists_to_sparse_bow(id_lists, vocab_size):\n",
    "    \"\"\"Convert a list-of-lists-of-ids to a sparse bag-of-words matrix.\n",
    "\n",
    "    Args:\n",
    "        id_lists: (list(list(int))) list of lists of word ids\n",
    "        vocab_size: (int) vocab size; must be greater than the largest word id\n",
    "            in id_lists.\n",
    "\n",
    "    Returns:\n",
    "        (scipy.sparse.csr_matrix) where each row is a sparse vector of word\n",
    "        counts for the corresponding example.\n",
    "    \"\"\"\n",
    "    from scipy import sparse\n",
    "    ii = []  # row indices (example ids)\n",
    "    jj = []  # column indices (token ids)\n",
    "    for row_id, ids in enumerate(id_lists):\n",
    "        ii.extend([row_id]*len(ids))\n",
    "        jj.extend(ids)\n",
    "    x = sparse.csr_matrix((np.ones_like(ii), (ii, jj)),\n",
    "                          shape=[len(id_lists), vocab_size])\n",
    "    return x\n",
    "\n",
    "def rnnlm_batch_generator(ids, batch_size, max_time):\n",
    "    \"\"\"Convert ids to data-matrix form for RNN language modeling.\"\"\"\n",
    "    # Clip to multiple of max_time for convenience\n",
    "    clip_len = ((len(ids)-1) // batch_size) * batch_size\n",
    "    input_w = ids[:clip_len]     # current word\n",
    "    target_y = ids[1:clip_len+1]  # next word\n",
    "    # Reshape so we can select columns\n",
    "    input_w = input_w.reshape([batch_size,-1])\n",
    "    target_y = target_y.reshape([batch_size,-1])\n",
    "\n",
    "    # Yield batches\n",
    "    for i in range(0, input_w.shape[1], max_time):\n",
    "        yield input_w[:,i:i+max_time], target_y[:,i:i+max_time]\n",
    "\n",
    "\n",
    "def build_windows(ids, N, shuffle=True):\n",
    "    \"\"\"Build window input to the window model.\n",
    "\n",
    "    Takes a sequence of ids, and returns a data matrix where each row\n",
    "    is a window and target for the window model. For N=3:\n",
    "        windows[i] = [w_3, w_2, w_1, w_0]\n",
    "\n",
    "    For language modeling, N is the context size and you can use y = windows[:,-1]\n",
    "    as the target words and x = windows[:,:-1] as the contexts.\n",
    "\n",
    "    For CBOW, N is the window size and you can use y = windows[:,N/2] as the target words\n",
    "    and x = np.hstack([windows[:,:N/2], windows[:,:N/2+1]]) as the contexts.\n",
    "\n",
    "    For skip-gram, you can use x = windows[:,N/2] as the input words and y = windows[:,i]\n",
    "    where i != N/2 as the target words.\n",
    "\n",
    "    Args:\n",
    "      ids: np.array(int32) of input ids\n",
    "      shuffle: if true, will randomly shuffle the rows\n",
    "\n",
    "    Returns:\n",
    "      windows: np.array(int32) of shape [len(ids)-N, N+1]\n",
    "        i.e. each row is a window, of length N+1\n",
    "    \"\"\"\n",
    "    windows = np.zeros((len(ids)-N, N+1), dtype=int)\n",
    "    for i in range(N+1):\n",
    "        # First column: first word, etc.\n",
    "        windows[:,i] = ids[i:len(ids)-(N-i)]\n",
    "    if shuffle:\n",
    "        # Shuffle rows\n",
    "        np.random.shuffle(windows)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def batch_generator(data, batch_size):\n",
    "    \"\"\"Generate minibatches from data.\n",
    "\n",
    "    Args:\n",
    "      data: array-like, supporting slicing along first dimension\n",
    "      batch_size: int, batch size\n",
    "\n",
    "    Yields:\n",
    "      minibatches of maximum size batch_size\n",
    "    \"\"\"\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i+batch_size]\n",
    "\n",
    "def multi_batch_generator(batch_size, *data_arrays):\n",
    "    \"\"\"Generate minibatches from multiple columns of data.\n",
    "\n",
    "    Example:\n",
    "        for (bx, by) in multi_batch_generator(5, x, y):\n",
    "            # bx is minibatch for x\n",
    "            # by is minibatch for y\n",
    "\n",
    "    Args:\n",
    "      batch_size: int, batch size\n",
    "      data_arrays: one or more array-like, supporting slicing along the first\n",
    "        dimension, and with matching first dimension.\n",
    "\n",
    "    Yields:\n",
    "      minibatches of maximum size batch_size\n",
    "    \"\"\"\n",
    "    assert(data_arrays)\n",
    "    num_examples = len(data_arrays[0])\n",
    "    for i in range(1, len(data_arrays)):\n",
    "        assert(len(data_arrays[i]) == num_examples)\n",
    "\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        # Yield matching slices from each data array.\n",
    "        yield tuple(data[i:i+batch_size] for data in data_arrays)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9UNOeDfKU-WY"
   },
   "source": [
    "### Loading in the data\n",
    "\n",
    "Data from AES dataset is stored in the `/data/` folder.  We will begin by loading the training dataset `training_set_rel3.tsv` and partitioning it into train, test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "IRtbnmPSegW5",
    "outputId": "c42dbbd1-3b7a-4722-e866-1658c106f091"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gUOU-wOOfGwJ",
    "outputId": "e03dc58b-f260-44bd-c7aa-c71801b2ec64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/content/drive/My Drive/Colab Notebooks/590project/data/training_set_rel3.cav.csv'\n"
     ]
    }
   ],
   "source": [
    "!ls \"/content/drive/My Drive/Colab Notebooks/590project/data/training_set_rel3.cav.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Hidaps9bU-WY",
    "outputId": "cde2c85a-ca06-4b60-9e24-ad06158b1c9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows in full data set: 12978\n"
     ]
    }
   ],
   "source": [
    "training_set_rel3_df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/590project/data/training_set_rel3.cav.csv\")\n",
    "#training_set_rel3_df.head()\n",
    "print(\"No. of rows in full data set:\", len(training_set_rel3_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "id": "KgjPOGuhU-Wa",
    "outputId": "858f76fd-f471-420d-dc52-9728daa8f6c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>rater1_trait1</th>\n",
       "      <th>rater1_trait2</th>\n",
       "      <th>rater1_trait3</th>\n",
       "      <th>rater1_trait4</th>\n",
       "      <th>rater1_trait5</th>\n",
       "      <th>rater1_trait6</th>\n",
       "      <th>rater2_trait1</th>\n",
       "      <th>rater2_trait2</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set  ... rater3_trait5  rater3_trait6\n",
       "0         1          1  ...           NaN            NaN\n",
       "1         2          1  ...           NaN            NaN\n",
       "2         3          1  ...           NaN            NaN\n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_rel3_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lMNhA0iuU-Wc",
    "outputId": "6c054130-9855-46a9-815b-a95fd474527e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows in full data set: 12977\n"
     ]
    }
   ],
   "source": [
    "# Dropping data with no scores in \"domain1_score\"\n",
    "training_set_rel3_df.dropna(axis=0, subset=['domain1_score'], inplace=True)\n",
    "print(\"No. of rows in full data set:\", len(training_set_rel3_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N3taqA5iU-We"
   },
   "outputs": [],
   "source": [
    "# Setting up the prompts\n",
    "prompt_dict = {1 : 'More and more people use computers, but not everyone agrees that this benefits society. Those who support advances in technology believe that computers have a positive effect on people. They teach hand-eye coordination, give people the ability to learn about faraway places and people, and even allow people to talk online with other people. Others have different ideas. Some experts are concerned that people are spending too much time on their computers and less time exercising, enjoying nature, and interacting with family and friends. Write a letter to your local newspaper in which you state your opinion on the effects computers have on people. Persuade the readers to agree with you.',\n",
    "               2 : 'Censorship in the Libraries. \"All of us can think of a book that we hope none of our children or any other children have taken off the shelf. But if I have the right to remove that book from the shelf -- that work I abhor -- then you also have exactly the same right and so does everyone else. And then we have no books left on the shelf for any of us.\" --Katherine Paterson, Author. Write a persuasive essay to a newspaper reflecting your views on censorship in libraries. Do you believe that certain materials, such as books, music, movies, magazines, etc., should be removed from the shelves if they are found offensive? Support your position with convincing arguments from your own experience, observations, and/or reading.',\n",
    "               3 : 'ROUGH ROAD AHEAD: Do Not Exceed Posted Speed Limit -- by Joe Kurmaskie. FORGET THAT OLD SAYING ABOUT NEVER taking candy from strangers. No, a better piece of advice for the solo cyclist would be, “Never accept travel advice from a collection of old-timers who haven’t left the confines of their porches since Carter was in office.” It’s not that a group of old guys doesn’t know the terrain. With age comes wisdom and all that, but the world is a fluid place. Things change. At a reservoir campground outside of Lodi, California, I enjoyed the serenity of an early-summer evening and some lively conversation with these old codgers. What I shouldn’t have done was let them have a peek at my map. Like a foolish youth, the next morning I followed their advice and launched out at first light along a “shortcut” that was to slice away hours from my ride to Yosemite National Park. They’d sounded so sure of themselves when pointing out landmarks and spouting off towns I would come to along this breezy jaunt. Things began well enough. I rode into the morning with strong legs and a smile on my face. About forty miles into the pedal, I arrived at the first “town.” This place might have been a thriving little spot at one time—say, before the last world war—but on that morning it fit the traditional definition of a ghost town. I chuckled, checked my water supply, and moved on. The sun was beginning to beat down, but I barely noticed it. The cool pines and rushing rivers of Yosemite had my name written all over them. Twenty miles up the road, I came to a fork of sorts. One ramshackle shed, several rusty pumps, and a corral that couldn’t hold in the lamest mule greeted me. This sight was troubling. I had been hitting my water bottles pretty regularly, and I was traveling through the high deserts of California in June. I got down on my hands and knees, working the handle of the rusted water pump with all my strength. A tarlike substance oozed out, followed by brackish water feeling somewhere in the neighborhood of two hundred degrees. I pumped that handle for several minutes, but the water wouldn’t cool down. It didn’t matter. When I tried a drop or two, it had the flavor of battery acid. The old guys had sworn the next town was only eighteen miles down the road. I could make that! I would conserve my water and go inward for an hour or so—a test of my inner spirit. Not two miles into this next section of the ride, I noticed the terrain changing. Flat road was replaced by short, rolling hills. After I had crested the first few of these, a large highway sign jumped out at me. It read: ROUGH ROAD AHEAD: DO NOT EXCEED POSTED SPEED LIMIT. The speed limit was 55 mph. I was doing a water-depleting 12 mph. Sometimes life can feel so cruel. I toiled on. At some point, tumbleweeds crossed my path and a ridiculously large snake—it really did look like a diamondback—blocked the majority of the pavement in front of me. I eased past, trying to keep my balance in my dehydrated state. The water bottles contained only a few tantalizing sips. Wide rings of dried sweat circled my shirt, and the growing realization that I could drop from heatstroke on a gorgeous day in June simply because I listened to some gentlemen who hadn’t been off their porch in decades, caused me to laugh. It was a sad, hopeless laugh, mind you, but at least I still had the energy to feel sorry for myself. There was no one in sight, not a building, car, or structure of any kind. I began breaking the ride down into distances I could see on the horizon, telling myself that if I could make it that far, I’d be fine. Over one long, crippling hill, a building came into view. I wiped the sweat from my eyes to make sure it wasn’t a mirage, and tried not to get too excited. With what I believed was my last burst of energy, I maneuvered down the hill. In an ironic twist that should please all sadists reading this, the building—abandoned years earlier, by the looks of it—had been a Welch’s Grape Juice factory and bottling plant. A sandblasted picture of a young boy pouring a refreshing glass of juice into his mouth could still be seen. I hung my head. That smoky blues tune “Summertime” rattled around in the dry honeycombs of my deteriorating brain. I got back on the bike, but not before I gathered up a few pebbles and stuck them in my mouth. I’d read once that sucking on stones helps take your mind off thirst by allowing what spit you have left to circulate. With any luck I’d hit a bump and lodge one in my throat. It didn’t really matter. I was going to die and the birds would pick me clean, leaving only some expensive outdoor gear and a diary with the last entry in praise of old men, their wisdom, and their keen sense of direction. I made a mental note to change that paragraph if it looked like I was going to lose consciousness for the last time. Somehow, I climbed away from the abandoned factory of juices and dreams, slowly gaining elevation while losing hope. Then, as easily as rounding a bend, my troubles, thirst, and fear were all behind me. GARY AND WILBER’S FISH CAMP—IF YOU WANT BAIT FOR THE BIG ONES, WE’RE YOUR BEST BET! “And the only bet,” I remember thinking. As I stumbled into a rather modern bathroom and drank deeply from the sink, I had an overwhelming urge to seek out Gary and Wilber, kiss them, and buy some bait—any bait, even though I didn’t own a rod or reel. An old guy sitting in a chair under some shade nodded in my direction. Cool water dripped from my head as I slumped against the wall beside him. “Where you headed in such a hurry?” “Yosemite,” I whispered. “Know the best way to get there?” I watched him from the corner of my eye for a long moment. He was even older than the group I’d listened to in Lodi. “Yes, sir! I own a very good map.” And I promised myself right then that I’d always stick to it in the future. “Rough Road Ahead” by Joe Kurmaskie, from Metal Cowboy, copyright © 1999 Joe Kurmaskie. Write a response that explains how the features of the setting affect the cyclist. In your response, include examples from the essay that support your conclusion.',\n",
    "               4 : 'Winter Hibiscus by Minfong Ho Saeng, a teenage girl, and her family have moved to the United States from Vietnam. As Saeng walks home after failing her driver’s test, she sees a familiar plant. Later, she goes to a florist shop to see if the plant can be purchased. It was like walking into another world. A hot, moist world exploding with greenery. Huge flat leaves, delicate wisps of tendrils, ferns and fronds and vines of all shades and shapes grew in seemingly random profusion. “Over there, in the corner, the hibiscus. Is that what you mean?” The florist pointed at a leafy potted plant by the corner. There, in a shaft of the wan afternoon sunlight, was a single blood-red blossom, its five petals splayed back to reveal a long stamen tipped with yellow pollen. Saeng felt a shock of recognition so intense, it was almost visceral. “Saebba,” Saeng whispered. A saebba hedge, tall and lush, had surrounded their garden, its lush green leaves dotted with vermilion flowers. And sometimes after a monsoon rain, a blossom or two would have blown into the well, so that when she drew the well water, she would find a red blossom floating in the bucket. Slowly, Saeng walked down the narrow aisle toward the hibiscus. Orchids, lanna bushes, oleanders, elephant ear begonias, and bougainvillea vines surrounded her. Plants that she had not even realized she had known but had forgotten drew her back into her childhood world. When she got to the hibiscus, she reached out and touched a petal gently. It felt smooth and cool, with a hint of velvet toward the center—just as she had known it would feel. And beside it was yet another old friend, a small shrub with waxy leaves and dainty flowers with purplish petals and white centers. “Madagascar periwinkle,” its tag announced. How strange to see it in a pot, Saeng thought. Back home it just grew wild, jutting out from the cracks in brick walls or between tiled roofs. And that rich, sweet scent—that was familiar, too. Saeng scanned the greenery around her and found a tall, gangly plant with exquisite little white blossoms on it.  “Dok Malik,” she said, savoring the feel of the word on her tongue, even as she silently noted the English name on its tag, “jasmine.” One of the blossoms had fallen off, and carefully Saeng picked it up and smelled it. She closed her eyes and breathed in, deeply. The familiar fragrance filled her lungs, and Saeng could almost feel the light strands of her grandmother’s long gray hair, freshly washed, as she combed it out with the fine-toothed buffalo-horn comb. And when the sun had dried it, Saeng would help the gnarled old fingers knot the hair into a bun, then slip a dok Malik bud into it. Saeng looked at the white bud in her hand now, small and fragile. Gently, she closed her palm around it and held it tight. That, at least, she could hold on to. But where was the fine-toothed comb? The hibiscus hedge? The well? Her gentle grandmother? A wave of loss so deep and strong that it stung Saeng’s eyes now swept over her. A blink, a channel switch, a boat ride into the night, and it was all gone. Irretrievably, irrevocably gone. And in the warm moist shelter of the greenhouse, Saeng broke down and wept. It was already dusk when Saeng reached home. The wind was blowing harder, tearing off the last remnants of green in the chicory weeds that were growing out of the cracks in the sidewalk. As if oblivious to the cold, her mother was still out in the vegetable garden, digging up the last of the onions with a rusty trowel. She did not see Saeng until the girl had quietly knelt down next to her. Her smile of welcome warmed Saeng. “Ghup ma laio le? You’re back?” she said cheerfully. “Goodness, it’s past five. What took you so long? How did it go? Did you—?” Then she noticed the potted plant that Saeng was holding, its leaves quivering in the wind. Mrs. Panouvong uttered a small cry of surprise and delight. “Dok faeng-noi!” she said. “Where did you get it?” “I bought it,” Saeng answered, dreading her mother’s next question. “How much?” For answer Saeng handed her mother some coins. “That’s all?” Mrs. Panouvong said, appalled, “Oh, but I forgot! You and the Lambert boy ate Bee-Maags . . . .” “No, we didn’t, Mother,” Saeng said. “Then what else—?” “Nothing else. I paid over nineteen dollars for it.” “You what?” Her mother stared at her incredulously. “But how could you? All the seeds for this vegetable garden didn’t cost that much! You know how much we—” She paused, as she noticed the tearstains on her daughter’s cheeks and her puffy eyes. “What happened?” she asked, more gently. “I—I failed the test,” Saeng said. For a long moment Mrs. Panouvong said nothing. Saeng did not dare look her mother in the eye. Instead, she stared at the hibiscus plant and nervously tore off a leaf, shredding it to bits. Her mother reached out and brushed the fragments of green off Saeng’s hands. “It’s a beautiful plant, this dok faeng-noi,” she finally said. “I’m glad you got it.” “It’s—it’s not a real one,” Saeng mumbled. “I mean, not like the kind we had at—at—” She found that she was still too shaky to say the words at home, lest she burst into tears again. “Not like the kind we had before,” she said. “I know,” her mother said quietly. “I’ve seen this kind blooming along the lake. Its flowers aren’t as pretty, but it’s strong enough to make it through the cold months here, this winter hibiscus. That’s what matters.” She tipped the pot and deftly eased the ball of soil out, balancing the rest of the plant in her other hand. “Look how root-bound it is, poor thing,” she said. “Let’s plant it, right now.” She went over to the corner of the vegetable patch and started to dig a hole in the ground. The soil was cold and hard, and she had trouble thrusting the shovel into it. Wisps of her gray hair trailed out in the breeze, and her slight frown deepened the wrinkles around her eyes. There was a frail, wiry beauty to her that touched Saeng deeply. “Here, let me help, Mother,” she offered, getting up and taking the shovel away from her. Mrs. Panouvong made no resistance. “I’ll bring in the hot peppers and bitter melons, then, and start dinner. How would you like an omelet with slices of the bitter melon?” “I’d love it,” Saeng said. Left alone in the garden, Saeng dug out a hole and carefully lowered the “winter hibiscus” into it. She could hear the sounds of cooking from the kitchen now, the beating of eggs against a bowl, the sizzle of hot oil in the pan. The pungent smell of bitter melon wafted out, and Saeng’s mouth watered. It was a cultivated taste, she had discovered—none of her classmates or friends, not even Mrs. Lambert, liked it—this sharp, bitter melon that left a golden aftertaste on the tongue. But she had grown up eating it and, she admitted to herself, much preferred it to a Big Mac. The “winter hibiscus” was in the ground now, and Saeng tamped down the soil around it. Overhead, a flock of Canada geese flew by, their faint honks clear and—yes—familiar to Saeng now. Almost reluctantly, she realized that many of the things that she had thought of as strange before had become, through the quiet repetition of season upon season, almost familiar to her now. Like the geese. She lifted her head and watched as their distinctive V was etched against the evening sky, slowly fading into the distance. When they come back, Saeng vowed silently to herself, in the spring, when the snows melt and the geese return and this hibiscus is budding, then I will take that test again. “Winter Hibiscus” by Minfong Ho, copyright © 1993 by Minfong Ho, from Join In, Multiethnic Short Stories, by Donald R. Gallo, ed. Read the last paragraph of the story. \"When they come back, Saeng vowed silently to herself, in the spring, when the snows melt and the geese return and this hibiscus is budding, then I will take that test again.\" Write a response that explains why the author concludes the story with this paragraph. In your response, include details and examples from the story that support your ideas.',\n",
    "               5 : 'Narciso Rodriguez from Home: The Blueprints of Our Lives My parents, originally from Cuba, arrived in the United States in 1956. After living for a year in a furnished one-room apartment, twenty-one-year-old Rawedia Maria and twenty-seven-year-old Narciso Rodriguez, Sr., could afford to move into a modest, three-room apartment I would soon call home. In 1961, I was born into this simple house, situated in a two-family, blond-brick building in the Ironbound section of Newark, New Jersey. Within its walls, my young parents created our traditional Cuban home, the very heart of which was the kitchen. My parents both shared cooking duties and unwittingly passed on to me their rich culinary skills and a love of cooking that is still with me today (and for which I am eternally grateful). Passionate Cuban music (which I adore to this day) filled the air, mixing with the aromas of the kitchen. Here, the innocence of childhood, the congregation of family and friends, and endless celebrations that encompassed both, formed the backdrop to life in our warm home. Growing up in this environment instilled in me a great sense that “family” had nothing to do with being a blood relative. Quite the contrary, our neighborhood was made up of mostly Spanish, Cuban, and Italian immigrants at a time when overt racism was the norm and segregation prevailed in the United States. In our neighborhood, despite customs elsewhere, all of these cultures came together in great solidarity and friendship. It was a close-knit community of honest, hardworking immigrants who extended a hand to people who, while not necessarily their own kind, were clearly in need. Our landlord and his daughter, Alegria (my babysitter and first friend), lived above us, and Alegria graced our kitchen table for meals more often than not. Also at the table were Sergio and Edelmira, my surrogate grandparents who lived in the basement apartment. (I would not know my “real” grandparents, Narciso the Elder and Consuelo, until 1970 when they were allowed to leave Cuba.) My aunts Bertha and Juanita and my cousins Arnold, Maria, and Rosemary also all lived nearby and regularly joined us at our table. Countless extended family members came and went — and there was often someone staying with us temporarily until they were able to get back on their feet. My parents always kept their arms and their door open to the many people we considered family, knowing that they would do the same for us. My mother and father had come to this country with such courage, without any knowledge of the language or the culture. They came selflessly, as many immigrants do, to give their children a better life, even though it meant leaving behind their families, friends, and careers in the country they loved. They struggled both personally and financially, braving the harsh northern winters while yearning for their native tropics and facing cultural hardships. The barriers to work were strong and high, and my parents both had to accept that they might not be able to find the kind of jobs they deserved. In Cuba, Narciso, Sr., had worked in a laboratory and Rawedia Maria had studied chemical engineering. In the United States, they had to start their lives over entirely, taking whatever work they could find. The faith that this struggle would lead them and their children to better times drove them to endure these hard times. I will always be grateful to my parents for their love and sacrifice. I’ve often told them that what they did was a much more courageous thing than I could have ever done. I’ve often told them of my admiration for their strength and perseverance, and I’ve thanked them repeatedly. But, in reality, there is no way to express my gratitude for the spirit of generosity impressed upon me at such an early age and the demonstration of how important family and friends are. These are two lessons that my parents did not just tell me. They showed me with their lives, and these teachings have been the basis of my life. It was in this simple house that my parents welcomed other refugees to celebrate their arrival to this country and where I celebrated my first birthdays. It was in the warmth of the kitchen in this humble house where a Cuban feast (albeit a frugal Cuban feast) always filled the air with not just scent and music but life and love. It was here where I learned the real definition of “family.” And for this, I will never forget that house or its gracious neighborhood or the many things I learned there about how to love. I will never forget how my parents turned this simple house into a home. — Narciso Rodriguez, Fashion designer. Hometown: Newark, New Jersey. “Narciso Rodriguez” by Narciso Rodriguez, from Home: The Blueprints of Our Lives. Copyright © 2006 by John Edwards. Describe the mood created by the author in the memoir. Support your answer with relevant and specific information from the memoir.',\n",
    "               6 : 'The Mooring Mast by Marcia Amidon Lüsted. When the Empire State Building was conceived, it was planned as the world’s tallest building, taller even than the new Chrysler Building that was being constructed at Forty-second Street and Lexington Avenue in New York. At seventy-seven stories, it was the tallest building before the Empire State began construction, and Al Smith was determined to outstrip it in height. The architect building the Chrysler Building, however, had a trick up his sleeve. He secretly constructed a 185-foot spire inside the building, and then shocked the public and the media by hoisting it up to the top of the Chrysler Building, bringing it to a height of 1,046 feet, 46 feet taller than the originally announced height of the Empire State Building. Al Smith realized that he was close to losing the title of world’s tallest building, and on December 11, 1929, he announced that the Empire State would now reach the height of 1,250 feet. He would add a top or a hat to the building that would be even more distinctive than any other building in the city. John Tauranac describes the plan: [The top of the Empire State Building] would be more than ornamental, more than a spire or dome or a pyramid put there to add a desired few feet to the height of the building or to mask something as mundane as a water tank. Their top, they said, would serve a higher calling. The Empire State Building would be equipped for an age of transportation that was then only the dream of aviation pioneers. This dream of the aviation pioneers was travel by dirigible, or zeppelin, and the Empire State Building was going to have a mooring mast at its top for docking these new airships, which would accommodate passengers on already existing transatlantic routes and new routes that were yet to come. The Age of Dirigibles. By the 1920s, dirigibles were being hailed as the transportation of the future. Also known today as blimps, dirigibles were actually enormous steel-framed balloons, with envelopes of cotton fabric filled with hydrogen and helium to make them lighter than air. Unlike a balloon, a dirigible could be maneuvered by the use of propellers and rudders, and passengers could ride in the gondola, or enclosed compartment, under the balloon.Dirigibles had a top speed of eighty miles per hour, and they could cruise at seventy miles per hour for thousands of miles without needing refueling. Some were as long as one thousand feet, the same length as four blocks in New York City. The one obstacle to their expanded use in New York City was the lack of a suitable landing area. Al Smith saw an opportunity for his Empire State Building: A mooring mast added to the top of the building would allow dirigibles to anchor there for several hours for refueling or service, and to let passengers off and on. Dirigibles were docked by means of an electric winch, which hauled in a line from the front of the ship and then tied it to a mast. The body of the dirigible could swing in the breeze, and yet passengers could safely get on and off the dirigible by walking down a gangplank to an open observation platform. The architects and engineers of the Empire State Building consulted with experts, taking tours of the equipment and mooring operations at the U.S. Naval Air Station in Lakehurst, New Jersey. The navy was the leader in the research and development of dirigibles in the United States. The navy even offered its dirigible, the Los Angeles, to be used in testing the mast. The architects also met with the president of a recently formed airship transport company that planned to offer dirigible service across the Pacific Ocean. When asked about the mooring mast, Al Smith commented: [It’s] on the level, all right. No kidding. We’re working on the thing now. One set of engineers here in New York is trying to dope out a practical, workable arrangement and the Government people in Washington are figuring on some safe way of mooring airships to this mast. Designing the Mast. The architects could not simply drop a mooring mast on top of the Empire State Building’s flat roof. A thousand-foot dirigible moored at the top of the building, held by a single cable tether, would add stress to the building’s frame. The stress of the dirigible’s load and the wind pressure would have to be transmitted all the way to the building’s foundation, which was nearly eleven hundred feet below. The steel frame of the Empire State Building would have to be modified and strengthened to accommodate this new situation. Over sixty thousand dollars’ worth of modifications had to be made to the building’s framework. Rather than building a utilitarian mast without any ornamentation, the architects designed a shiny glass and chrome-nickel stainless steel tower that would be illuminated from inside, with a stepped-back design that imitated the overall shape of the building itself. The rocket-shaped mast would have four wings at its corners, of shiny aluminum, and would rise to a conical roof that would house the mooring arm. The winches and control machinery for the dirigible mooring would be housed in the base of the shaft itself, which also housed elevators and stairs to bring passengers down to the eighty-sixth floor, where baggage and ticket areas would be located. The building would now be 102 floors, with a glassed-in observation area on the 101st floor and an open observation platform on the 102nd floor. This observation area was to double as the boarding area for dirigible passengers. Once the architects had designed the mooring mast and made changes to the existing plans for the building’s skeleton, construction proceeded as planned. When the building had been framed to the 85th floor, the roof had to be completed before the framing for the mooring mast could take place. The mast also had a skeleton of steel and was clad in stainless steel with glass windows. Two months after the workers celebrated framing the entire building, they were back to raise an American flag again—this time at the top of the frame for the mooring mast. The Fate of the Mast. The mooring mast of the Empire State Building was destined to never fulfill its purpose, for reasons that should have been apparent before it was ever constructed. The greatest reason was one of safety: Most dirigibles from outside of the United States used hydrogen rather than helium, and hydrogen is highly flammable. When the German dirigible Hindenburg was destroyed by fire in Lakehurst, New Jersey, on May 6, 1937, the owners of the Empire State Building realized how much worse that accident could have been if it had taken place above a densely populated area such as downtown New York. The greatest obstacle to the successful use of the mooring mast was nature itself. The winds on top of the building were constantly shifting due to violent air currents. Even if the dirigible were tethered to the mooring mast, the back of the ship would swivel around and around the mooring mast. Dirigibles moored in open landing fields could be weighted down in the back with lead weights, but using these at the Empire State Building, where they would be dangling high above pedestrians on the street, was neither practical nor safe. The other practical reason why dirigibles could not moor at the Empire State Building was an existing law against airships flying too low over urban areas. This law would make it illegal for a ship to ever tie up to the building or even approach the area, although two dirigibles did attempt to reach the building before the entire idea was dropped. In December 1930, the U.S. Navy dirigible Los Angeles approached the mooring mast but could not get close enough to tie up because of forceful winds. Fearing that the wind would blow the dirigible onto the sharp spires of other buildings in the area, which would puncture the dirigible’s shell, the captain could not even take his hands off the control levers. Two weeks later, another dirigible, the Goodyear blimp Columbia, attempted a publicity stunt where it would tie up and deliver a bundle of newspapers to the Empire State Building. Because the complete dirigible mooring equipment had never been installed, a worker atop the mooring mast would have to catch the bundle of papers on a rope dangling from the blimp. The papers were delivered in this fashion, but after this stunt the idea of using the mooring mast was shelved. In February 1931, Irving Clavan of the building’s architectural office said, “The as yet unsolved problems of mooring air ships to a fixed mast at such a height made it desirable to postpone to a later date the final installation of the landing gear.” By the late 1930s, the idea of using the mooring mast for dirigibles and their passengers had quietly disappeared. Dirigibles, instead of becoming the transportation of the future, had given way to airplanes. The rooms in the Empire State Building that had been set aside for the ticketing and baggage of dirigible passengers were made over into the world’s highest soda fountain and tea garden for use by the sightseers who flocked to the observation decks. The highest open observation deck, intended for disembarking passengers, has never been open to the public. “The Mooring Mast” by Marcia Amidon Lüsted, from The Empire State Building. Copyright © 2004 by Gale, a part of Cengage Learning, Inc. Based on the excerpt, describe the obstacles the builders of the Empire State Building faced in attempting to allow dirigibles to dock there. Support your answer with relevant and specific information from the excerpt.',\n",
    "               7 : 'Write about patience. Being patient means that you are understanding and tolerant. A patient person experience difficulties without complaining. Do only one of the following: write a story about a time when you were patient OR write a story about a time when someone you know was patient OR write a story in your own way about patience.',\n",
    "               8 : 'We all understand the benefits of laughter. For example, someone once said, “Laughter is the shortest distance between two people.” Many other people believe that laughter is an important part of any relationship. Tell a true story in which laughter was one element or part.'\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VGdWLjYkU-Wg"
   },
   "outputs": [],
   "source": [
    "# Adding prompts column into the dataframe\n",
    "training_set_rel3_df.insert(loc=2, column=\"prompt\", \n",
    "                            value=[prompt_dict[i] for i in training_set_rel3_df[\"essay_set\"]])\n",
    "\n",
    "# Adding the normalized scores column into the dataframe\n",
    "training_set_rel3_df.insert(loc=4, column=\"scores\", \n",
    "                            value=[(row[\"domain1_score\"]-2)/10 if row[\"essay_set\"]==1 else \n",
    "                                   (row[\"domain1_score\"]-1 + row[\"domain2_score\"]-1)/(5+3) if row[\"essay_set\"]==2 else\n",
    "                                   (row[\"domain1_score\"])/3 if row[\"essay_set\"]==3 else\n",
    "                                   (row[\"domain1_score\"])/3 if row[\"essay_set\"]==4 else\n",
    "                                   (row[\"domain1_score\"])/4 if row[\"essay_set\"]==5 else\n",
    "                                   (row[\"domain1_score\"])/4 if row[\"essay_set\"]==6 else\n",
    "                                   (row[\"domain1_score\"])/30 if row[\"essay_set\"]==7 else\n",
    "                                   (row[\"domain1_score\"])/60\n",
    "                                   for index, row in training_set_rel3_df.iterrows() \n",
    "                                  ]\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ob0mbBBHU-Wh"
   },
   "outputs": [],
   "source": [
    "# Tokenization and stripping @*** (@CAPS1, @ORGANIZATION1) using the TweetTokenizer with strip_handles=True\n",
    "# Also convert to lowercase and strip punctuation (except @***)\n",
    "tknzr = nltk.tokenize.TweetTokenizer(strip_handles=True)\n",
    "dropped_punctuation = ''.join(s for s in string.punctuation if s!='@')\n",
    "dropped_punctuation = string.punctuation + '”“’—©'\n",
    "\n",
    "# Tokenizing the prompts\n",
    "tk_prompt_dict = {i:tknzr.tokenize(prompt_dict[i]) for i in prompt_dict.keys()}\n",
    "# Removing punctuation\n",
    "for i in tk_prompt_dict.keys():\n",
    "    tk_prompt_dict[i] = list(map(lambda x:x.translate(str.maketrans('', '', dropped_punctuation)).lower(),tk_prompt_dict[i]))\n",
    "    tk_prompt_dict[i] = [str(w).replace(' ','') for w in tk_prompt_dict[i]]\n",
    "    tk_prompt_dict[i] = [s for s in tk_prompt_dict[i] if s]\n",
    "\n",
    "# Adding a column for tokenized prompt\n",
    "training_set_rel3_df.insert(loc=5, column=\"tk_prompt\", \n",
    "                            value=[tk_prompt_dict[i] for i in training_set_rel3_df[\"essay_set\"]])\n",
    "\n",
    "# Tokenizing the essays\n",
    "tk_essay_list = [tknzr.tokenize(row[\"essay\"]) for index, row in training_set_rel3_df.iterrows()]\n",
    "# Removing punctuation\n",
    "for i in range(len(tk_essay_list)):\n",
    "    tk_essay_list[i] = list(map(lambda x:x.translate(str.maketrans('', '', dropped_punctuation)).lower(),tk_essay_list[i]))\n",
    "    tk_essay_list[i] = [str(w).replace(' ','') for w in tk_essay_list[i]]\n",
    "    tk_essay_list[i] = [s for s in tk_essay_list[i] if s]\n",
    "\n",
    "# Adding a column for tokenized essay\n",
    "training_set_rel3_df.insert(loc=6, column=\"tk_essay\", \n",
    "                            value=[tk_essay_list[i] for i in range(len(tk_essay_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "id": "8UqWdD2CU-Wj",
    "outputId": "a1ac4cb7-497f-4e0f-efb7-dbac1972bb54"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>prompt</th>\n",
       "      <th>essay</th>\n",
       "      <th>scores</th>\n",
       "      <th>tk_prompt</th>\n",
       "      <th>tk_essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>rater1_trait1</th>\n",
       "      <th>rater1_trait2</th>\n",
       "      <th>rater1_trait3</th>\n",
       "      <th>rater1_trait4</th>\n",
       "      <th>rater1_trait5</th>\n",
       "      <th>rater1_trait6</th>\n",
       "      <th>rater2_trait1</th>\n",
       "      <th>rater2_trait2</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12975</th>\n",
       "      <td>21629</td>\n",
       "      <td>8</td>\n",
       "      <td>We all understand the benefits of laughter. Fo...</td>\n",
       "      <td>When you laugh, is @CAPS5 out of habit, or is ...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[we, all, understand, the, benefits, of, laugh...</td>\n",
       "      <td>[when, you, laugh, is, out, of, habit, or, is,...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12976</th>\n",
       "      <td>21630</td>\n",
       "      <td>8</td>\n",
       "      <td>We all understand the benefits of laughter. Fo...</td>\n",
       "      <td>Trippin' on fen...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[we, all, understand, the, benefits, of, laugh...</td>\n",
       "      <td>[trippin, on, fences, i, am, years, young, and...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12977</th>\n",
       "      <td>21633</td>\n",
       "      <td>8</td>\n",
       "      <td>We all understand the benefits of laughter. Fo...</td>\n",
       "      <td>Many people believe that laughter can improve...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[we, all, understand, the, benefits, of, laugh...</td>\n",
       "      <td>[many, people, believe, that, laughter, can, i...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id  essay_set  ... rater3_trait5 rater3_trait6\n",
       "12975     21629          8  ...           4.0           4.0\n",
       "12976     21630          8  ...           NaN           NaN\n",
       "12977     21633          8  ...           NaN           NaN\n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_rel3_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "ba_pzRnOU-Wl",
    "outputId": "4c604a67-5ecf-432c-f415-732a79f4adc0",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4EAAAEWCAYAAADYVpIeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 1080x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting histogram for prompt and essay token lengths\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,4))\n",
    "fig.subplots_adjust(wspace=.4)\n",
    "\n",
    "prompt_len = [len(training_set_rel3_df[\"tk_prompt\"][index]) for index, row in training_set_rel3_df.iterrows()]\n",
    "essay_len = [len(training_set_rel3_df[\"tk_essay\"][index]) for index, row in training_set_rel3_df.iterrows()]\n",
    "\n",
    "ax[0].hist(x=prompt_len, bins=50, edgecolor=\"w\")\n",
    "ax[0].set_title(\"Histogram of Prompt Length (Tokens)\")\n",
    "ax[0].set_xlabel(\"No. of Tokens\")\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "\n",
    "ax[1].hist(x=essay_len, bins=50, edgecolor=\"w\")\n",
    "ax[1].set_title(\"Histogram of Essay Length (Tokens)\")\n",
    "ax[1].set_xlabel(\"No. of Tokens\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "c0nw3VJaU-Wn",
    "outputId": "03424a9f-32a8-4095-ec44-ea1dc4a3cff2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt lengths = {1153, 836, 45, 111, 1649, 1460, 58, 127}\n",
      "Setting max_len_p = 130\n",
      "\n",
      "Setting max_len = 650, \n",
      " 96.8% of essays are shorter than this length.\n"
     ]
    }
   ],
   "source": [
    "# Setting parameters for max_len\n",
    "print(\"Prompt lengths =\", set(prompt_len))\n",
    "max_len_p = 130\n",
    "print(\"Setting max_len_p =\", max_len_p)\n",
    "print()\n",
    "\n",
    "max_len = 650\n",
    "print(\"Setting max_len = {}, \\n {:.1f}% of essays are shorter than this length.\"\n",
    "      .format(max_len, 100*sum(i <= max_len for i in essay_len) / len(training_set_rel3_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lb5QyTuEU-Wp"
   },
   "outputs": [],
   "source": [
    "## NEW VERSION -- with BERT trunking at 512 words\n",
    "\n",
    "# Padding and trunking for array export\n",
    "# Padding will be done at the ending for prompts, but trunking will be at the begining for length > max_len_p. \n",
    "training_set_rel3_df.insert(loc=7, column=\"tk_pad_prompt\", \n",
    "                            value=[row[\"tk_prompt\"][-max_len_p:] if len(row[\"tk_prompt\"])>max_len_p else\n",
    "                                   row[\"tk_prompt\"] + [\"<pad>\" for n in range(max_len_p - len(row[\"tk_prompt\"]))]  \n",
    "                                   if len(row[\"tk_prompt\"])<max_len_p else\n",
    "                                   row[\"tk_prompt\"]\n",
    "                                   for index, row in training_set_rel3_df.iterrows() \n",
    "                                  ]\n",
    "                           )\n",
    "\n",
    "# Padding and trunking will be done at the ending for essays. \n",
    "training_set_rel3_df.insert(loc=8, column=\"tk_pad_essay\", \n",
    "                            value=[row[\"tk_essay\"][:max_len] if len(row[\"tk_essay\"])>max_len else\n",
    "                                   row[\"tk_essay\"] + [\"<pad>\" for n in range(max_len - len(row[\"tk_essay\"]))] \n",
    "                                   if len(row[\"tk_essay\"])<max_len else\n",
    "                                   row[\"tk_essay\"]\n",
    "                                   for index, row in training_set_rel3_df.iterrows() \n",
    "                                  ]\n",
    "                           )\n",
    "\n",
    "# Adding columns for the length of the truncated prompt and the essay to identify no. of words before padding\n",
    "training_set_rel3_df.insert(loc=9, column=\"nw_prompt\", \n",
    "                            value=[len(row[\"tk_prompt\"]) if len(row[\"tk_prompt\"])<max_len_p else\n",
    "                                   max_len_p\n",
    "                                   for index, row in training_set_rel3_df.iterrows() \n",
    "                                  ]\n",
    "                           )\n",
    "\n",
    "training_set_rel3_df.insert(loc=10, column=\"nw_essay\", \n",
    "                            value=[len(row[\"tk_essay\"]) if len(row[\"tk_essay\"])<max_len else\n",
    "                                   max_len\n",
    "                                   for index, row in training_set_rel3_df.iterrows() \n",
    "                                  ]\n",
    "                           )\n",
    "\n",
    "# Adding columns for only truncated prompt and essay, for later BERT embeddings max length is 512\n",
    "max_len_bert = 512\n",
    "\n",
    "training_set_rel3_df.insert(loc=11, column=\"tk_trunc_prompt\", \n",
    "                            value=[row[\"tk_prompt\"][-max_len_p:] if len(row[\"tk_prompt\"])>max_len_p else\n",
    "                                   row[\"tk_prompt\"]\n",
    "                                   for index, row in training_set_rel3_df.iterrows() \n",
    "                                  ]\n",
    "                           )\n",
    "\n",
    "training_set_rel3_df.insert(loc=12, column=\"tk_trunc_essay\", \n",
    "                            value=[row[\"tk_essay\"][:max_len_bert] if len(row[\"tk_essay\"])>max_len_bert else\n",
    "                                   row[\"tk_essay\"]\n",
    "                                   for index, row in training_set_rel3_df.iterrows() \n",
    "                                  ]\n",
    "                           )\n",
    "\n",
    "# Checks for the length of the tokens\n",
    "assert set([len(training_set_rel3_df[\"tk_pad_prompt\"][index]) \n",
    "            for index, row in training_set_rel3_df.iterrows()])==set([max_len_p]), \"Checks for Prompt padding fail\"\n",
    "assert set([len(training_set_rel3_df[\"tk_pad_essay\"][index])\n",
    "            for index, row in training_set_rel3_df.iterrows()])==set([max_len]), \"Checks for Essay padding fail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GIEPOyeoU-Wq"
   },
   "outputs": [],
   "source": [
    "# Creating train, dev and test sets\n",
    "train_set, test_set = train_test_split(training_set_rel3_df, test_size=0.1, random_state=0)\n",
    "train_set, dev_set = train_test_split(train_set, test_size=15/90, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "WfbDRz6EU-Ws",
    "outputId": "dae7c6e8-758f-412a-874e-ee02149b3d50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of examples in Train Set : 9732\n",
      "No. of examples in Dev Set   : 1947\n",
      "No. of examples in Test Set  : 1298\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of examples in Train Set :\", len(train_set))\n",
    "print(\"No. of examples in Dev Set   :\", len(dev_set))\n",
    "print(\"No. of examples in Test Set  :\", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ST39p0zU-Wu"
   },
   "outputs": [],
   "source": [
    "# Extracting out as arrays, Note: still need to convert to ids based on the embedding\n",
    "train_set_prompts = np.array(list(train_set[\"tk_pad_prompt\"]))\n",
    "train_set_essays = np.array(list(train_set[\"tk_pad_essay\"]))\n",
    "train_set_labels = np.array(list(train_set[\"scores\"]))\n",
    "\n",
    "dev_set_prompts = np.array(list(dev_set[\"tk_pad_prompt\"]))\n",
    "dev_set_essays = np.array(list(dev_set[\"tk_pad_essay\"]))\n",
    "dev_set_labels = np.array(list(dev_set[\"scores\"]))\n",
    "\n",
    "test_set_prompts = np.array(list(test_set[\"tk_pad_prompt\"]))\n",
    "test_set_essays = np.array(list(test_set[\"tk_pad_essay\"]))\n",
    "test_set_labels = np.array(list(test_set[\"scores\"]))\n",
    "\n",
    "# # Separate extraction for BERT implementation which are only truncated to max length 512, no padding at this stage.\n",
    "# train_set_prompts_bert = np.array(list(train_set[\"tk_trunc_prompt\"])) \n",
    "# train_set_essays_bert = np.array(list(train_set[\"tk_trunc_essay\"]))\n",
    "# train_set_labels = np.array(list(train_set[\"scores\"]))\n",
    "\n",
    "# dev_set_prompts_bert = np.array(list(dev_set[\"tk_trunc_prompt\"]))\n",
    "# dev_set_essays_bert = np.array(list(dev_set[\"tk_trunc_essay\"]))\n",
    "# dev_set_labels = np.array(list(dev_set[\"scores\"]))\n",
    "\n",
    "# test_set_prompts_bert = np.array(list(test_set[\"tk_trunc_prompt\"]))\n",
    "# test_set_essays_bert = np.array(list(test_set[\"tk_trunc_essay\"]))\n",
    "# test_set_labels = np.array(list(test_set[\"scores\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mLjSLSrFU-Wv"
   },
   "outputs": [],
   "source": [
    "# Extracting out the no. of words prior to padding just in case\n",
    "train_set_prompt_nw = np.array(list(train_set[\"nw_prompt\"]))\n",
    "train_set_essay_nw = np.array(list(train_set[\"nw_essay\"]))\n",
    "\n",
    "dev_set_prompt_nw = np.array(list(dev_set[\"nw_prompt\"]))\n",
    "dev_set_essay_nw = np.array(list(dev_set[\"nw_essay\"]))\n",
    "\n",
    "test_set_prompt_nw = np.array(list(test_set[\"nw_prompt\"]))\n",
    "test_set_essay_nw = np.array(list(test_set[\"nw_essay\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "kboj-h0ZU-Wx",
    "outputId": "2fa20b7c-6f90-4651-a454-8ea365757f01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'mood', 'in', 'the', 'memoir', 'that', 'the', 'author',\n",
       "       'creates', 'is'], dtype='<U35')"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the array shape\n",
    "assert train_set_essays.shape == (len(train_set),max_len), \"Checks for array shape fail\"\n",
    "\n",
    "#Checking first 10 tokens of index 0 of train_set_essays\n",
    "train_set_essays[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VcfoR9zDU-Wz"
   },
   "source": [
    "### Baseline\n",
    "\n",
    "A baseline was established with Neural Bag of Words\n",
    "\n",
    "![Neural Bag-of-Words Model](neural_bow.png)\n",
    "\n",
    "Using the following notation:\n",
    "- $w^{(i)} \\in \\mathbb{Z}$ for the $i^{th}$ word of the sequence (as an integer index)\n",
    "- $x^{(i)} \\in \\mathbb{R}^d$ for the vector representation (embedding) of $w^{(i)}$\n",
    "- $x \\in \\mathbb{R}^d$ for the fixed-length vector given by summing all the $x^{(i)}$ for an example\n",
    "- $h^{(j)}$ for the hidden state after the $j^{th}$ fully-connected layer\n",
    "- $y$ for the target label ($\\in 1,\\ldots,\\mathtt{num\\_classes}$)\n",
    "\n",
    "Our model is defined as:\n",
    "- **Embedding layer:** $x^{(i)} = W_{embed}[w^{(i)}]$\n",
    "- **Summing vectors:** $x = \\sum_{i=1}^n x^{(i)}$\n",
    "- **Hidden layer(s):** $h^{(j)} = f(h^{(j-1)} W^{(j)} + b^{(j)})$ where $h^{(-1)} = x$ and $j = 0,1,\\ldots,J-1$\n",
    "- **Output layer:** $\\hat{y} = \\hat{P}(y) = \\mathrm{softmax}(h^{(final)} W_{out} + b_{out})$ where $h^{(final)} = h^{(J-1)}$ is the output of the last hidden layer.\n",
    "\n",
    "Logits for the softmax is defined as:\n",
    "$$ \\mathrm{logits} = h^{(final)}W_{out} + b_{out} $$\n",
    "\n",
    "Other dimensions:\n",
    "- `V`: the vocabulary size\n",
    "- `embed_dim`: the embedding dimension $d$\n",
    "- `hidden_dims`: a list of dimensions for the output of each hidden layer (i.e. $\\mathrm{dim}(h^{(j)})$&nbsp;=&nbsp;`hidden_dims[j]`)\n",
    "- `num_classes`: the number of target classes (2 for the binary task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Z6Rw3-2aU-Wz",
    "outputId": "bc6c84c7-f396-4193-9de6-3dd618fac2b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary is 13382\n"
     ]
    }
   ],
   "source": [
    "# Building the Vocabulary\n",
    "\n",
    "# Set the threshold number of occurences to prune words from the vocab\n",
    "prune_threshold = 3\n",
    "\n",
    "# Defining sorting and unique functions\n",
    "def uniq(lst):\n",
    "    last = object()\n",
    "    for item in lst:\n",
    "        if item == last:\n",
    "            continue\n",
    "        yield item\n",
    "        last = item\n",
    "\n",
    "def sort_and_deduplicate(lst):\n",
    "    return list(uniq(sorted(lst, reverse=False)))\n",
    "\n",
    "def word_ctr(lst):\n",
    "    ctr_list = []\n",
    "    last = object()\n",
    "    for word in lst:\n",
    "        if word == last:\n",
    "            ctr_list[-1] += 1\n",
    "        else:\n",
    "            ctr_list.append(1)\n",
    "        last = word\n",
    "    return ctr_list\n",
    "\n",
    "def prune(lst, ctr_lst, num=10):\n",
    "    assert len(lst) == len(ctr_lst), \"List must have same length\"\n",
    "    for i in range(len(lst)):\n",
    "        if ctr_lst[i] < num and ctr_lst[i] != \"<pad>\":\n",
    "            lst[i] = None\n",
    "    return [w for w in lst if w is not None]\n",
    "\n",
    "# Add in words from prompts and essays\n",
    "words = []\n",
    "for key in tk_prompt_dict.keys():\n",
    "    words += tk_prompt_dict[key]\n",
    "for index, row in training_set_rel3_df.iterrows():\n",
    "    words += [w for w in row[\"tk_essay\"]]\n",
    "\n",
    "vocab = sort_and_deduplicate(words)\n",
    "# Pruning words which have less than the threshold occurences\n",
    "vocab_ctr = word_ctr(sorted(words, reverse=False))\n",
    "vocab = prune(vocab, vocab_ctr , num=prune_threshold) \n",
    "# Pre-load in the words for padding and unknown,\n",
    "vocab = [\"<pad>\", \"<unk>\"] + vocab\n",
    "\n",
    "# Removing numbers but not <pad> or <unk>\n",
    "vocab = [w for w in vocab if not any(char.isdigit() for char in w)]\n",
    "\n",
    "# Checking if there are any \"@***\" not parsed out\n",
    "for index, row in training_set_rel3_df.iterrows():\n",
    "    for w in row[\"tk_essay\"]:\n",
    "        if '@' in w:\n",
    "            print(\"Issues with index\", index)\n",
    "            \n",
    "# Defining a dictionary for the vocabulary\n",
    "vocab_dict = {}\n",
    "for ctr in range(len(vocab)):\n",
    "    vocab_dict[vocab[ctr]] = ctr\n",
    "\n",
    "# Defining a get word id function\n",
    "def get_vocab_id(word):\n",
    "    if word in vocab_dict.keys():\n",
    "        return vocab_dict[word]\n",
    "    else:\n",
    "        return vocab_dict[\"<unk>\"]\n",
    "       \n",
    "# Defining a get word from id function\n",
    "def get_vocab_word(ids):\n",
    "    for key, value in vocab_dict.items():\n",
    "        if ids == value: \n",
    "            return key\n",
    "        else:\n",
    "            return \"<unk>\"\n",
    "\n",
    "# Defining a fuction to convert arrays (train_set_essay / train_set_prompts) into word ids\n",
    "def get_id_array(arr):\n",
    "    arr_ids = np.array(list(list(map(get_vocab_id, row)) for row in arr))\n",
    "    return arr_ids\n",
    "\n",
    "# Setting the size of the Vocabulary\n",
    "V = len(vocab_dict)\n",
    "print(\"The size of the vocabulary is\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ulH_7thU-W1"
   },
   "outputs": [],
   "source": [
    "# Obtaining the word ids for feeding into the model\n",
    "train_set_prompt_ids = get_id_array(train_set_prompts)\n",
    "train_set_essay_ids = get_id_array(train_set_essays)\n",
    "\n",
    "dev_set_prompt_ids = get_id_array(dev_set_prompts)\n",
    "dev_set_essay_ids = get_id_array(dev_set_essays)\n",
    "\n",
    "test_set_prompt_ids = get_id_array(test_set_prompts)\n",
    "test_set_essay_ids = get_id_array(test_set_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HHqOTc8WU-W4",
    "outputId": "78ad4fdf-3b5d-4fd9-8fef-3d15d21cb33b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of class for the classification: 1\n"
     ]
    }
   ],
   "source": [
    "# For regression:\n",
    "num_score_classes = 1\n",
    "print(\"No. of class for the classification:\", num_score_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzsdXXp9U-W9"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8J7kHUK-U-W-"
   },
   "outputs": [],
   "source": [
    "# Defining the various layers in the NBOW model\n",
    "\n",
    "def embedding_layer(ids_, V, embed_dim, init_scale=0.001):\n",
    "    \"\"\"Construct an embedding layer.\n",
    "    Args:\n",
    "        ids_: [batch_size, max_len] Tensor of int32, integer ids\n",
    "        V: (int) vocabulary size\n",
    "        embed_dim: (int) embedding dimension\n",
    "        init_scale: (float) scale to initialize embeddings\n",
    "\n",
    "    Returns:\n",
    "        xs_: [batch_size, max_len, embed_dim] Tensor of float32, embeddings for\n",
    "            each element in ids_\n",
    "    \"\"\"\n",
    "    ## Assigning variables\n",
    "    W_embed_ = tf.get_variable('W_embed', shape=[V, embed_dim], dtype=tf.float32,\n",
    "                          initializer=tf.random_uniform_initializer(-init_scale, init_scale))\n",
    "    \n",
    "    ## Looking up embeddings\n",
    "    xs_ = tf.nn.embedding_lookup(params=W_embed_, ids=ids_)\n",
    "\n",
    "    return xs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WNMhdrKHU-XA"
   },
   "outputs": [],
   "source": [
    "def fully_connected_layers(h0_, hidden_dims, activation=tf.tanh,\n",
    "                           dropout_rate=0, is_training=False):\n",
    "    \"\"\"Construct a stack of fully-connected layers.\n",
    "\n",
    "    Args:\n",
    "        h0_: [batch_size, d] Tensor of float32, the input activations\n",
    "        hidden_dims: list(int) dimensions of the output of each layer\n",
    "        activation: TensorFlow function, such as tf.tanh. Passed to\n",
    "            tf.layers.dense.\n",
    "        dropout_rate: if > 0, will apply dropout to activations.\n",
    "        is_training: (bool) if true, is in training mode\n",
    "\n",
    "    Returns:\n",
    "        h_: [batch_size, hidden_dims[-1]] Tensor of float32, the activations of\n",
    "            the last layer constructed by this function.\n",
    "    \"\"\"\n",
    "    h_ = h0_\n",
    "    for i, hdim in enumerate(hidden_dims):\n",
    "        h_ = tf.layers.dense(h_, hdim, activation=activation, name=(\"Hidden_%d\"%i))\n",
    "\n",
    "        if dropout_rate > 0:\n",
    "            h_ = tf.layers.dropout(h_, rate=dropout_rate, training=is_training) \n",
    "\n",
    "    return h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VGDL9qpXU-XB"
   },
   "outputs": [],
   "source": [
    "def mse_output_layer(h_, labels_, num_classes):\n",
    "    \"\"\"Construct a mean squared error output layer.\n",
    "\n",
    "    Implements:\n",
    "        output = h W + b\n",
    "        loss = tf.losses.mean_squared_error(labels=labels_, predictions=predictions)\n",
    "\n",
    "    Args:\n",
    "        h_: [batch_size, d] Tensor of float32, the input activations from a\n",
    "            previous layer\n",
    "        labels_: [batch_size] Tensor of float32, the target label ids\n",
    "        num_classes: (int) the number of output classes\n",
    "\n",
    "    Returns: (loss_, predictions_)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"Predictions\"):\n",
    "        \n",
    "        ##Assigning variables\n",
    "        W_out_ = tf.get_variable(\"W_out\", shape=[h_.shape[-1], num_classes], \n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.random_normal_initializer())\n",
    "        b_out_ = tf.get_variable(\"b_out\", shape=[num_classes,],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.zeros_initializer())\n",
    "        # Calculating the predictions\n",
    "        predictions_ = tf.matmul(h_, W_out_) + b_out_\n",
    "        \n",
    "    # If no labels provided, don't try to compute loss.\n",
    "    if labels_ is None:\n",
    "         return None, predictions_\n",
    "\n",
    "    with tf.name_scope(\"MSE\"):\n",
    "        loss_ = tf.losses.mean_squared_error(labels=labels_, predictions=tf.reshape(predictions_, [-1]), weights=1.0)\n",
    "\n",
    "    return loss_, predictions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FqrflwJpU-XD"
   },
   "outputs": [],
   "source": [
    "def BOW_encoder(ids_, ns_, V, embed_dim, hidden_dims, dropout_rate=0,\n",
    "                is_training=None,\n",
    "                **unused_kw):\n",
    "    \"\"\"Construct a bag-of-words encoder.\n",
    "\n",
    "        - Build the embeddings (using embedding_layer(...))\n",
    "        - Apply the mask to zero-out padding indices, and sum the embeddings\n",
    "            for each example\n",
    "        - Build a stack of hidden layers (using fully_connected_layers(...))\n",
    "\n",
    "    Args:\n",
    "        ids_: [batch_size, max_len] Tensor of int32, integer ids\n",
    "        ns_:  [batch_size] Tensor of int32, (clipped) length of each sequence\n",
    "        V: (int) vocabulary size\n",
    "        embed_dim: (int) embedding dimension\n",
    "        hidden_dims: list(int) dimensions of the output of each layer\n",
    "        dropout_rate: (float) rate to use for dropout\n",
    "        is_training: (bool) if true, is in training mode\n",
    "\n",
    "    Returns: (h_, xs_)\n",
    "        h_: [batch_size, hidden_dims[-1]] Tensor of float32, the activations of\n",
    "            the last layer constructed by this function.\n",
    "        xs_: [batch_size, max_len, embed_dim] Tensor of float32, the per-word\n",
    "            embeddings as returned by embedding_layer and with the mask applied\n",
    "            to zero-out the pad indices.\n",
    "    \"\"\"\n",
    "    assert is_training is not None, \"is_training must be explicitly set to True or False\"\n",
    "    # Embedding layer:\n",
    "    #   xs_: [batch_size, max_len, embed_dim]\n",
    "    with tf.variable_scope(\"Embedding_Layer\"):\n",
    "         xs_ = embedding_layer(ids_, V, embed_dim, init_scale=0.001)\n",
    "\n",
    "    # Mask off the padding indices with zeros\n",
    "    #   mask_: [batch_size, max_len, 1] with values of 0.0 or 1.0\n",
    "    mask_ = tf.expand_dims(tf.sequence_mask(ns_, xs_.shape[1],\n",
    "                                           dtype=tf.float32), -1)\n",
    "    # Multiply xs_ by the mask to zero-out pad indices.\n",
    "    xs_ = tf.multiply(xs_, mask_)\n",
    "       \n",
    "    # Sum embeddings:\n",
    "    h0_ = tf.math.reduce_sum(xs_, axis=1)\n",
    "          \n",
    "    # Fully-connected layers\n",
    "    h_ = fully_connected_layers(h0_, hidden_dims, activation=tf.tanh,\n",
    "                                dropout_rate=dropout_rate, is_training=is_training)\n",
    "    \n",
    "    return h_, xs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RP8N6imcU-XF"
   },
   "outputs": [],
   "source": [
    "def regression_model_fn(features, labels, mode, params):\n",
    "       \n",
    "    # Check if this graph is going to be used for training.\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    if params['encoder_type'] == 'bow':\n",
    "        with tf.variable_scope(\"Encoder\"):\n",
    "            h_, xs_ = BOW_encoder(features['ids'], features['ns'],\n",
    "                                  is_training=is_training,\n",
    "                                  **params)\n",
    "    else:\n",
    "        raise ValueError(\"Error: unsupported encoder type \"\n",
    "                         \"'{:s}'\".format(params['encoder_type']))\n",
    "\n",
    "    # Construct output mse layer and loss functions\n",
    "    with tf.variable_scope(\"Output_Layer\"):\n",
    "        mse_loss_, predictions_ = mse_output_layer(h_, labels, params['num_classes'])\n",
    "\n",
    "    with tf.name_scope(\"Prediction\"):\n",
    "        predictions_dict = {\"y_hat\": predictions_}\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # If predict mode, don't bother computing loss.\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=predictions_dict)\n",
    "\n",
    "    # L2 regularization (weight decay) on parameters, from all layers\n",
    "    with tf.variable_scope(\"Regularization\"):\n",
    "        l2_penalty_ = tf.nn.l2_loss(xs_)  # l2 loss on embeddings\n",
    "        for var_ in tf.trainable_variables():\n",
    "            if \"Embedding_Layer\" in var_.name:\n",
    "                continue\n",
    "            l2_penalty_ += tf.nn.l2_loss(var_)\n",
    "        l2_penalty_ *= params['beta']  # scale by regularization strength\n",
    "        tf.summary.scalar(\"l2_penalty\", l2_penalty_)\n",
    "        regularized_loss_ = mse_loss_ + l2_penalty_\n",
    "\n",
    "    with tf.variable_scope(\"Training\"):\n",
    "        if params['optimizer'] == 'adagrad':\n",
    "            optimizer_ = tf.train.AdagradOptimizer(params['lr'])\n",
    "        else:\n",
    "            optimizer_ = tf.train.GradientDescentOptimizer(params['lr'])\n",
    "        train_op_ = optimizer_.minimize(regularized_loss_,\n",
    "                                        global_step=tf.train.get_global_step())\n",
    "        \n",
    "    \n",
    "    tf.summary.scalar(\"mse_loss\", mse_loss_)\n",
    "    eval_metrics = {\"mse_loss\": tf.metrics.mean(mse_loss_)}\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      predictions=predictions_dict,\n",
    "                                      loss=regularized_loss_,\n",
    "                                      train_op=train_op_,\n",
    "                                      eval_metric_ops=eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "45W4Oqx0U-XG",
    "outputId": "8c4ddfe3-231e-4e82-bc40-d29795a4eeb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To view training (once it starts), run:\n",
      "\n",
      "    tensorboard --logdir='/tmp/project/tf_baseline_nbow20191130-2339' --port 6006\n",
      "\n",
      "Then in your browser, open: http://localhost:6006\n"
     ]
    }
   ],
   "source": [
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=V, embed_dim=100, hidden_dims=[50], num_classes=num_score_classes,\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.01)\n",
    "\n",
    "# Setting the directory to save the checkpoints to\n",
    "checkpoint_dir = \"/tmp/project/tf_baseline_nbow\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=regression_model_fn, \n",
    "                               params=model_params,\n",
    "                               model_dir=checkpoint_dir)\n",
    "\n",
    "print(\"\")\n",
    "print(\"To view training (once it starts), run:\\n\")\n",
    "print(\"    tensorboard --logdir='{:s}' --port 6006\".format(checkpoint_dir))\n",
    "print(\"\\nThen in your browser, open: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "43Lt-2g5U-XI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training params for the input_fn to tf.estimator\n",
    "batch_size = 32\n",
    "train_params = dict(batch_size=batch_size, total_epochs=20, eval_every=2)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "# Using patch_numpy_io.numpy_input_fn instead of tf.estimator.inputs.numpy_input_fn\n",
    "train_input_fn = numpy_input_fn(\n",
    "                    x={\"ids\": train_set_essay_ids, \"ns\": train_set_essay_nw}, y=train_set_labels,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True\n",
    "                 )\n",
    "\n",
    "# Input function for dev set batches.\n",
    "dev_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": dev_set_essay_ids, \"ns\": dev_set_essay_nw}, y=dev_set_labels,\n",
    "                    batch_size=batch_size, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "    # Train for a few epochs, then evaluate on dev\n",
    "    model.train(input_fn=train_input_fn)\n",
    "    eval_metrics = model.evaluate(input_fn=dev_input_fn, name=\"dev\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ie7ik_tRU-XK"
   },
   "outputs": [],
   "source": [
    "# Function to calculate Quadratic Weighted Kappa (QWK), from the labels and the predictions\n",
    "def get_QWK(float_score_list, predict_dict):\n",
    "\n",
    "    def convert_0_100_scores(float_labels_list):\n",
    "        return [int(i*100) for i in float_labels_list]\n",
    "\n",
    "    def convert_prediction_dict_to_lst(predict_dict):\n",
    "        predict_lst = []\n",
    "        for d in predict_dict:\n",
    "            predict_lst.append(d[\"y_hat\"][0])\n",
    "        return predict_lst\n",
    "    \n",
    "    labels_int = convert_0_100_scores(list(float_score_list))\n",
    "    predict_int = convert_0_100_scores(convert_prediction_dict_to_lst(predict_dict))\n",
    "    \n",
    "    return sklearn.metrics.cohen_kappa_score(labels_int, predict_int, weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "eihs_Ku6U-XM",
    "outputId": "df432c84-7034-4b56-d2ef-384ee6c4989f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural BOW baseline for MSE on dev set: 2.74%\n",
      "Neural BOW baseline for QWK on dev set: 0.7557\n"
     ]
    }
   ],
   "source": [
    "predict_dev = list(model.predict(input_fn=dev_input_fn))\n",
    "print(\"Neural BOW baseline for MSE on dev set: {:.02%}\".format(eval_metrics['mse_loss']))\n",
    "print(\"Neural BOW baseline for QWK on dev set: {:.04}\".format(get_QWK(dev_set_labels, predict_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "tax06sh4U-XR",
    "outputId": "68338c96-459a-4429-d7a1-50cf2bcf1c4f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neural BOW baseline for MSE on test set: 2.33%\n",
      "Neural BOW baseline for QWK on test set: 0.7777\n"
     ]
    }
   ],
   "source": [
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": test_set_essay_ids, \"ns\": test_set_essay_nw}, y=test_set_labels,\n",
    "                    batch_size=batch_size, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "eval_metrics = model.evaluate(input_fn=test_input_fn, name=\"test\")\n",
    "predict_test = list(model.predict(input_fn=test_input_fn))\n",
    "\n",
    "print()\n",
    "print(\"Neural BOW baseline for MSE on test set: {:.02%}\".format(eval_metrics['mse_loss']))\n",
    "print(\"Neural BOW baseline for QWK on test set: {:.04}\".format(get_QWK(test_set_labels, predict_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s86zxi46U-XS"
   },
   "source": [
    "### Baseline Results\n",
    "\n",
    "Based on running on 7 Apr:    \n",
    "\n",
    "    Neural BOW baseline for MSE on dev set: 3.83%\n",
    "    Neural BOW baseline for QWK on dev set: 0.7111\n",
    "    \n",
    "    Neural BOW baseline for MSE on test set: 3.53%    \n",
    "    Neural BOW baseline for QWK on test set: 0.7265"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KxsxvX13U-YX"
   },
   "source": [
    "### END"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "essay_scoring-mse-final.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
